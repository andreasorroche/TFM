{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "882c7827-d492-48aa-8cb6-a77614297265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we choose the pretrained model to use \n",
    "\n",
    "model_id = \"google/vit-base-patch16-224\"\n",
    "#model_id = 'microsoft/swin-tiny-patch4-window7-224'\n",
    "#model_id = 'facebook/deit-base-patch16-224'\n",
    "#model_id = 'microsoft/resnet-50'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f530c-0b87-4a91-82f4-a60bf626cdb9",
   "metadata": {},
   "source": [
    "Now we load the feature extractor to process the image into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02cc9044-cb70-47c5-b31f-1657558d1df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading feature extractor configuration file https://huggingface.co/google/vit-base-patch16-224/resolve/main/preprocessor_config.json from cache at /home/studio-lab-user/.cache/huggingface/transformers/caa0e8430c8ba68a0586cef2a661b39ea04de291f092f7c4277fc2d97f10cdb9.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, ViTFeatureExtractor, DeiTFeatureExtractor\n",
    "#feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_id)\n",
    "#feature_extractor = DeiTFeatureExtractor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4166d1c-1a73-4190-8e82-624200047301",
   "metadata": {},
   "source": [
    "This feature extractor will resize every image to the resolution that the model expects and normalize channels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491140c-c230-4655-9f89-e005c4d3a426",
   "metadata": {},
   "source": [
    "We define 2 functions, one for training and one for validation, including resizing, center cropping and normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd82a8a4-462b-4f15-ab70-da22f3a57f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(feature_extractor.size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(feature_extractor.size),\n",
    "            CenterCrop(feature_extractor.size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"img\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"img\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a291a-fafd-4580-b8ca-a40b9a71640b",
   "metadata": {},
   "source": [
    "Next, we can preprocess our dataset by applying these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9f48cb2-035f-4b64-844d-36911beb1f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 2050\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 228\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 402\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n",
    "ds = load_from_disk('./fillfactor_dict_0.7-0.02')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "300c5553-6b86-4086-ad35-a15c5875c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "train_ds = ds['train']\n",
    "val_ds = ds['val']\n",
    "test_ds = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "256e4b28-52e6-4572-886a-a2aaaf814b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proton', 'gamma']\n"
     ]
    }
   ],
   "source": [
    "#Classes names\n",
    "labels = train_ds.features[\"label\"].names\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97d151d8-791d-4eaf-ac20-0e22aab9454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfc4ed91-ffab-4e6f-bf0e-e90db0e8b8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=288x288>,\n",
       " 'label': 1,\n",
       " 'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaa161-cffe-4796-8012-91e66d879fd1",
   "metadata": {},
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. We use the modelViTForImageClassification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22aa812a-f91f-4b38-8b18-eab8d71163f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a dictionary that maps a label name to an integer and vice versa. \n",
    "#The mapping will help the model recover the label name from the label number.\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43fba3aa-f533-4ba3-ae49-a57b117002d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/vit-base-patch16-224/resolve/main/config.json from cache at /home/studio-lab-user/.cache/huggingface/transformers/6b03b61d64598274e01717c40e8909f9e70531219a281e8163bd5b3af5c92d1a.c41e6c561c79e9b15e74a5cc284a31cba59cb1a9e209933c1a04a46ba2e20e44\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"proton\",\n",
      "    \"1\": \"gamma\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"gamma\": \"1\",\n",
      "    \"proton\": \"0\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.20.1\"\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/vit-base-patch16-224/resolve/main/pytorch_model.bin from cache at /home/studio-lab-user/.cache/huggingface/transformers/2be66f502a86da34ba6c947f2c61e33e326b5ab34ee2213d634f6e7da1cafb69.2d858f78fc06693c373c41438a7379c6bc50f2c874d348ee505d8550393a5a88\n",
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, SwinForImageClassification,ResNetForImageClassification, TrainingArguments, Trainer, SegformerForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(model_id,\n",
    "                                                 label2id=label2id,\n",
    "                                                 id2label=id2label,\n",
    "                                                 ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "                                                 )\n",
    "\n",
    "#model = SwinForImageClassification.from_pretrained(model_id,\n",
    "#                                                 label2id=label2id,\n",
    "#                                                 id2label=id2label,\n",
    "#                                                 ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "#) \n",
    "\n",
    "#model = ResNetForImageClassification.from_pretrained(model_id,\n",
    "#                                                 label2id=label2id,\n",
    "#                                                 id2label=id2label,\n",
    "#                                                 ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "#)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3e17f-979c-47ca-bad5-76f087d1b333",
   "metadata": {},
   "source": [
    "The warning is telling us we are throwing away some weights (the weights and bias of the classifier layer) and randomly initializing some other (the weights and bias of a new classifier layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7ce9b-a0f0-4f10-90a1-77b8e5613dd1",
   "metadata": {},
   "source": [
    "To instantiate a Trainer, we will need to define the training configuration and the evaluation metric. The most important is the TrainingArguments, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model.\n",
    "\n",
    "Most of the training arguments are pretty self-explanatory, but one that is quite important here is remove_unused_columns=False. This one will drop any features not used by the model's call function. By default it's True because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in our case, we need the unused features ('image' in particular) in order to create 'pixel_values'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ede566e3-36e9-473d-baeb-30b0c2cf2ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_name = model_id.split(\"/\")[-1]\n",
    "batch_size = 32\n",
    "learning_rate = 5e-5\n",
    "gradient_accumulation_steps = 4\n",
    "epochs = 3\n",
    "warmup_ratio= 0.1\n",
    "logging_steps=10\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ds\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    logging_steps=logging_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "   # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1464cb-4b0a-4621-b65a-6056e36cad1c",
   "metadata": {},
   "source": [
    "Next, we need to define a function for how to compute the metrics from the predictions, which will just use the metric we loaded earlier. Let us also load the Accuracy metric, which we'll use to evaluate our model both during and after training. The only preprocessing we have to do is to take the argmax of our predicted logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be8d4103-623c-41be-816f-71aba0fad059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2b985-b913-4849-b3f4-2fdbe77fe455",
   "metadata": {},
   "source": [
    "We also define a collate_fn, which will be used to batch examples together. Each batch consists of 2 keys, namely pixel_values and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "649a749c-d111-47f8-807b-0ba04b53d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b265339-7eec-44e3-8c38-1ce884527253",
   "metadata": {},
   "source": [
    "Then we just need to pass all of this along with our datasets to the Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5780f1c8-20f9-4694-b90a-099b5b02a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9bc7b0-35ce-496f-a8ef-27f59326e421",
   "metadata": {},
   "source": [
    "Now we can finetune our model by calling the train method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9afe1d3f-c7e2-47a8-9411-5d8509d89f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/PRUEBA/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2050\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 53:32, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.665800</td>\n",
       "      <td>0.541117</td>\n",
       "      <td>0.728070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.483300</td>\n",
       "      <td>0.484872</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.456200</td>\n",
       "      <td>0.340108</td>\n",
       "      <td>0.894737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 228\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds/checkpoint-16\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/checkpoint-16/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/checkpoint-16/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/checkpoint-16/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 228\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds/checkpoint-32\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/checkpoint-32/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/checkpoint-32/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/checkpoint-32/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 228\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds/checkpoint-48\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/checkpoint-48/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/checkpoint-48/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/checkpoint-48/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from vit-base-patch16-224-finetuned-ds/checkpoint-48 (score: 0.8947368421052632).\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70610cf9-8570-4b37-b43c-bdc964a2c8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 228\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 01:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       2.98\n",
      "  eval_accuracy           =     0.8947\n",
      "  eval_loss               =     0.3401\n",
      "  eval_runtime            = 0:00:38.85\n",
      "  eval_samples_per_second =      5.868\n",
      "  eval_steps_per_second   =      0.206\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = trainer.evaluate()\n",
    "# some nice to haves:\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43c9156b-84a2-4d28-875f-6c9796f41d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96167b9c-d84b-43c0-a85f-fae3ccf58ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 402\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "outputs = trainer.predict(test_ds)\n",
    "y_pred = outputs.predictions.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a35c48e8-a78e-47d1-85f8-97fc7a5c91c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9079601990049752}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f847d7ab-5acd-454d-af4d-044049cab94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-4.46477473e-01,  4.98525172e-01],\n",
       "       [ 9.78303850e-01, -4.14050281e-01],\n",
       "       [ 1.92093587e+00, -1.66820168e+00],\n",
       "       [ 1.44558942e+00, -1.31376505e+00],\n",
       "       [ 2.91273981e-01, -3.13133150e-01],\n",
       "       [ 1.39345181e+00, -1.11967039e+00],\n",
       "       [ 1.51001453e-01, -1.90186992e-01],\n",
       "       [ 7.21857965e-01, -5.31801105e-01],\n",
       "       [ 2.96028495e-01, -2.86198884e-01],\n",
       "       [ 4.30927724e-01, -3.54968488e-01],\n",
       "       [ 4.89938259e-01, -4.73545313e-01],\n",
       "       [-7.49841332e-03,  1.40016779e-01],\n",
       "       [ 6.24491334e-01, -6.02689862e-01],\n",
       "       [-1.80947185e-02,  6.90227747e-02],\n",
       "       [ 6.24392450e-01, -5.79391778e-01],\n",
       "       [ 2.54645407e-01, -1.89287797e-01],\n",
       "       [ 4.79092181e-01, -3.78938556e-01],\n",
       "       [-2.54943967e-02,  5.35873771e-02],\n",
       "       [ 1.16291642e+00, -1.02925432e+00],\n",
       "       [-3.84330511e-01,  3.99698168e-01],\n",
       "       [ 1.18963957e-01, -8.70915651e-02],\n",
       "       [ 1.83616877e-02,  7.56940544e-02],\n",
       "       [-5.88526666e-01,  6.47658706e-01],\n",
       "       [ 1.31846142e+00, -8.42666388e-01],\n",
       "       [-2.47446120e-01,  2.75767535e-01],\n",
       "       [ 1.35415506e+00, -1.01496005e+00],\n",
       "       [-3.28262985e-01,  4.04586673e-01],\n",
       "       [ 4.54136372e-01, -2.75413483e-01],\n",
       "       [ 4.49220479e-01, -1.52695075e-01],\n",
       "       [ 1.56458020e-02, -1.74196064e-03],\n",
       "       [ 5.23691893e-01, -4.85266000e-01],\n",
       "       [ 8.16390574e-01, -7.55110204e-01],\n",
       "       [ 6.28096640e-01, -5.23776531e-01],\n",
       "       [ 1.13465750e+00, -8.30778837e-01],\n",
       "       [ 3.31674457e-01, -2.53936648e-01],\n",
       "       [-5.78017235e-01,  5.97942054e-01],\n",
       "       [ 9.25394177e-01, -8.49156618e-01],\n",
       "       [ 2.23937213e-01, -9.76831168e-02],\n",
       "       [ 1.30520606e+00, -1.18592191e+00],\n",
       "       [-9.43509936e-02,  2.25823179e-01],\n",
       "       [-1.61899447e-01,  8.73056054e-02],\n",
       "       [ 6.51231706e-02, -1.56940818e-02],\n",
       "       [ 9.90543664e-02,  2.42760032e-02],\n",
       "       [ 8.05726290e-01, -8.86344314e-01],\n",
       "       [ 4.86779332e-01, -3.63031983e-01],\n",
       "       [ 8.01985264e-02, -1.13436565e-01],\n",
       "       [ 1.13726807e+00, -8.95986855e-01],\n",
       "       [ 4.55713391e-01, -3.48884344e-01],\n",
       "       [ 4.05731887e-01, -3.34475398e-01],\n",
       "       [ 2.46680021e-01, -1.47560775e-01],\n",
       "       [ 1.03768563e+00, -8.62382531e-01],\n",
       "       [-7.58727789e-02,  1.03451997e-01],\n",
       "       [ 1.42278707e+00, -1.13384616e+00],\n",
       "       [ 1.74735045e+00, -1.27299333e+00],\n",
       "       [-7.23822415e-02,  1.18350580e-01],\n",
       "       [ 7.82007277e-02, -3.97354364e-02],\n",
       "       [ 9.15832102e-01, -7.83125162e-01],\n",
       "       [ 1.31504267e-01, -1.87719584e-01],\n",
       "       [ 3.05667520e-03,  9.18410867e-02],\n",
       "       [ 1.52226806e+00, -1.14605772e+00],\n",
       "       [ 7.73886442e-01, -6.81304693e-01],\n",
       "       [ 1.36684787e+00, -1.01501787e+00],\n",
       "       [ 4.01173949e-01, -4.16046321e-01],\n",
       "       [-8.90194178e-02,  3.40226591e-02],\n",
       "       [ 9.73223269e-01, -7.11304903e-01],\n",
       "       [ 1.12661839e+00, -9.83256459e-01],\n",
       "       [ 6.66201234e-01, -6.25630319e-01],\n",
       "       [-1.30328476e-01,  1.38565168e-01],\n",
       "       [ 1.67587316e+00, -1.43903506e+00],\n",
       "       [ 1.63588309e+00, -1.21774316e+00],\n",
       "       [ 1.03612685e+00, -9.78474259e-01],\n",
       "       [ 4.25109982e-01, -2.30386898e-01],\n",
       "       [-1.19410753e-01,  1.91451281e-01],\n",
       "       [ 2.63955593e-02,  6.51271641e-03],\n",
       "       [ 1.19490397e+00, -1.01060236e+00],\n",
       "       [ 8.60130847e-01, -7.99557030e-01],\n",
       "       [-3.63876313e-01,  3.19085062e-01],\n",
       "       [ 1.01036596e+00, -8.01831603e-01],\n",
       "       [ 2.41092592e-01, -2.56781548e-01],\n",
       "       [ 9.28895593e-01, -8.28063607e-01],\n",
       "       [ 1.19168788e-01, -1.23763964e-01],\n",
       "       [ 7.40704238e-02,  6.16851598e-02],\n",
       "       [ 1.58160543e+00, -1.11043286e+00],\n",
       "       [-2.60185450e-01,  2.59279609e-01],\n",
       "       [ 2.66656965e-01, -2.30791539e-01],\n",
       "       [ 1.58050513e+00, -1.31033611e+00],\n",
       "       [-3.55684847e-01,  4.44867581e-01],\n",
       "       [ 8.80696535e-01, -8.15982223e-01],\n",
       "       [-5.68954349e-01,  5.30218482e-01],\n",
       "       [ 1.63673234e+00, -1.34539032e+00],\n",
       "       [ 1.27595675e+00, -1.07963276e+00],\n",
       "       [ 6.01292849e-01, -5.87009370e-01],\n",
       "       [ 1.00137091e+00, -9.30361807e-01],\n",
       "       [-3.54786992e-01,  3.52053374e-01],\n",
       "       [ 1.47588193e+00, -1.21452606e+00],\n",
       "       [ 9.62736130e-01, -7.71364212e-01],\n",
       "       [ 1.70301884e-01, -1.62559584e-01],\n",
       "       [ 3.12081993e-01, -2.65314281e-01],\n",
       "       [ 7.13907063e-01, -5.55700660e-01],\n",
       "       [ 1.47683966e+00, -1.28025806e+00],\n",
       "       [ 2.21105635e-01, -1.33946419e-01],\n",
       "       [-1.50338769e-01,  2.95155466e-01],\n",
       "       [ 1.43497825e+00, -1.14583588e+00],\n",
       "       [ 1.27654409e+00, -1.07342076e+00],\n",
       "       [ 3.47913414e-01, -2.45371401e-01],\n",
       "       [ 1.37435651e+00, -1.09620976e+00],\n",
       "       [-6.67316556e-01,  7.55949557e-01],\n",
       "       [ 1.34091485e+00, -1.12149000e+00],\n",
       "       [-5.87542057e-02,  8.76740217e-02],\n",
       "       [ 1.17390037e+00, -1.04042077e+00],\n",
       "       [-5.98711967e-01,  6.10334516e-01],\n",
       "       [-7.21993089e-01,  7.54053473e-01],\n",
       "       [ 7.44632781e-01, -6.45408094e-01],\n",
       "       [-4.17549402e-01,  3.81708264e-01],\n",
       "       [ 7.24223256e-03,  1.82711810e-01],\n",
       "       [ 7.23124623e-01, -6.92447245e-01],\n",
       "       [-3.74113530e-01,  4.65615392e-01],\n",
       "       [ 7.11438000e-01, -7.76575625e-01],\n",
       "       [-1.13614708e-01,  1.10600889e-01],\n",
       "       [ 8.59522402e-01, -8.11827183e-01],\n",
       "       [-1.94844261e-01,  2.76818812e-01],\n",
       "       [-4.24303949e-01,  4.86484289e-01],\n",
       "       [ 3.11648250e-01, -1.67301446e-01],\n",
       "       [ 1.16658616e+00, -1.02452481e+00],\n",
       "       [ 5.84321499e-01, -5.85280418e-01],\n",
       "       [-5.04410565e-01,  5.66708863e-01],\n",
       "       [-6.58432245e-01,  7.18933940e-01],\n",
       "       [ 5.29648662e-01, -5.42872906e-01],\n",
       "       [ 1.64633930e+00, -1.33984661e+00],\n",
       "       [-5.32797635e-01,  5.70336044e-01],\n",
       "       [-6.20732009e-02,  5.96445501e-02],\n",
       "       [-2.44775042e-01,  3.76116902e-01],\n",
       "       [ 1.73073697e+00, -1.34435701e+00],\n",
       "       [ 1.27056682e+00, -1.07805645e+00],\n",
       "       [-3.93696487e-01,  4.47936684e-01],\n",
       "       [-3.04315180e-01,  3.56955171e-01],\n",
       "       [ 9.01741624e-01, -7.82998323e-01],\n",
       "       [ 4.54685330e-01, -4.19430971e-01],\n",
       "       [ 1.03113621e-01, -9.47922170e-02],\n",
       "       [ 1.21019554e+00, -1.05600321e+00],\n",
       "       [ 1.35901034e-01, -5.76296002e-02],\n",
       "       [-1.09838396e-01,  1.55194163e-01],\n",
       "       [ 3.15449536e-01, -2.67059296e-01],\n",
       "       [ 1.12689710e+00, -9.25355315e-01],\n",
       "       [ 1.71645248e+00, -1.39499497e+00],\n",
       "       [ 1.16404974e+00, -6.51732683e-01],\n",
       "       [ 7.40746856e-01, -6.64376140e-01],\n",
       "       [ 1.56843150e+00, -1.28958893e+00],\n",
       "       [ 5.40928364e-01, -3.56956959e-01],\n",
       "       [ 1.33804619e+00, -1.10738480e+00],\n",
       "       [ 8.42780471e-01, -6.19143546e-01],\n",
       "       [ 8.58640671e-01, -7.87427068e-01],\n",
       "       [-1.27615511e-01,  1.17351875e-01],\n",
       "       [ 1.86802197e+00, -1.27974117e+00],\n",
       "       [-2.62098014e-01,  3.52656215e-01],\n",
       "       [ 9.92489874e-01, -9.06688690e-01],\n",
       "       [-6.07562780e-01,  5.92009306e-01],\n",
       "       [ 9.74967957e-01, -9.39335465e-01],\n",
       "       [ 1.55522138e-01, -7.05364347e-02],\n",
       "       [ 6.38236284e-01, -6.36895299e-01],\n",
       "       [ 1.14775240e+00, -1.17646575e+00],\n",
       "       [ 1.65434098e+00, -1.12026715e+00],\n",
       "       [ 6.04941905e-01, -5.00978470e-01],\n",
       "       [ 9.27968681e-01, -8.25088739e-01],\n",
       "       [-2.72638857e-01,  3.08968186e-01],\n",
       "       [ 5.18158555e-01, -4.87117052e-01],\n",
       "       [ 6.41685486e-01, -6.03904247e-01],\n",
       "       [ 1.08891249e+00, -9.73962963e-01],\n",
       "       [ 1.37624478e+00, -1.08829129e+00],\n",
       "       [ 5.87946594e-01, -6.41327441e-01],\n",
       "       [ 6.92350149e-01, -6.18893802e-01],\n",
       "       [ 6.13472879e-01, -4.50948477e-01],\n",
       "       [ 1.85645640e-01, -1.66275397e-01],\n",
       "       [ 1.06470728e+00, -7.99160659e-01],\n",
       "       [ 1.44943094e+00, -9.89142656e-01],\n",
       "       [-6.74849153e-02,  1.79224595e-01],\n",
       "       [-4.69588161e-01,  5.01371682e-01],\n",
       "       [ 1.04078054e-01, -1.26308888e-01],\n",
       "       [ 8.51403892e-01, -4.14588213e-01],\n",
       "       [ 1.31653261e+00, -1.13193846e+00],\n",
       "       [ 1.17052853e+00, -9.98414040e-01],\n",
       "       [ 1.18994820e+00, -9.54501510e-01],\n",
       "       [ 3.38694751e-01, -3.44114989e-01],\n",
       "       [ 6.36386037e-01, -6.10017419e-01],\n",
       "       [ 1.34762526e-01, -1.42588362e-01],\n",
       "       [ 1.43586993e+00, -9.70215559e-01],\n",
       "       [ 7.19559073e-01, -7.58074999e-01],\n",
       "       [-1.74135983e-01,  3.13165992e-01],\n",
       "       [ 8.38160038e-01, -7.92497516e-01],\n",
       "       [-3.92019987e-01,  4.36006039e-01],\n",
       "       [ 1.03575432e+00, -9.62706447e-01],\n",
       "       [ 8.51579487e-01, -8.34807277e-01],\n",
       "       [ 5.97229540e-01, -5.09064615e-01],\n",
       "       [ 6.78022027e-01, -6.67642474e-01],\n",
       "       [-2.21617430e-01,  4.23836112e-01],\n",
       "       [-4.30909932e-01,  3.25803339e-01],\n",
       "       [ 1.41740286e+00, -1.30658948e+00],\n",
       "       [-5.34579933e-01,  6.31756783e-01],\n",
       "       [-2.30142936e-01,  2.46530265e-01],\n",
       "       [-4.81962055e-01,  5.49485862e-01],\n",
       "       [ 1.02420235e+00, -7.74083138e-01],\n",
       "       [-5.77412248e-02,  1.57199055e-01],\n",
       "       [ 1.00217080e+00, -7.01354742e-01],\n",
       "       [-1.72032028e-01,  2.45802522e-01],\n",
       "       [ 2.16635752e+00, -1.50340331e+00],\n",
       "       [ 1.11256969e+00, -9.49813128e-01],\n",
       "       [ 1.47467017e+00, -1.23455215e+00],\n",
       "       [ 3.07658970e-01, -3.91013414e-01],\n",
       "       [ 1.09235537e+00, -7.67026365e-01],\n",
       "       [ 1.24219477e+00, -1.04860902e+00],\n",
       "       [ 5.69693744e-02, -5.26436865e-02],\n",
       "       [ 1.29801142e+00, -8.37991238e-01],\n",
       "       [ 1.35344934e+00, -1.09306931e+00],\n",
       "       [-1.15235537e-01,  3.04332733e-01],\n",
       "       [ 2.82324970e-01, -1.93343833e-01],\n",
       "       [-5.25275767e-01,  5.54321289e-01],\n",
       "       [ 1.15469784e-01,  6.96554780e-03],\n",
       "       [ 3.99152398e-01, -3.65187854e-01],\n",
       "       [ 1.11934066e+00, -1.01447010e+00],\n",
       "       [ 1.16065323e-01, -1.29903346e-01],\n",
       "       [ 1.40584302e+00, -1.20591545e+00],\n",
       "       [ 2.26394117e-01, -1.99658930e-01],\n",
       "       [-2.03442663e-01,  2.51370132e-01],\n",
       "       [ 1.60544252e+00, -1.20838237e+00],\n",
       "       [ 1.72980332e+00, -1.38865471e+00],\n",
       "       [-4.32039380e-01,  4.51679558e-01],\n",
       "       [ 5.00712216e-01, -2.73832142e-01],\n",
       "       [ 5.99316835e-01, -6.06463730e-01],\n",
       "       [ 1.29602325e+00, -1.14067578e+00],\n",
       "       [-2.58507639e-01,  2.86157429e-01],\n",
       "       [ 1.56689179e+00, -1.38317728e+00],\n",
       "       [ 1.48598206e+00, -1.22616708e+00],\n",
       "       [-5.83570838e-01,  6.31089687e-01],\n",
       "       [ 1.21550250e+00, -1.01108181e+00],\n",
       "       [-7.67678857e-01,  7.82835305e-01],\n",
       "       [ 3.96106780e-01, -4.19292748e-01],\n",
       "       [ 7.87349343e-01, -7.36016512e-01],\n",
       "       [ 9.62249875e-01, -7.61086583e-01],\n",
       "       [ 1.10731769e+00, -6.49769902e-01],\n",
       "       [ 4.48100626e-01, -4.80171710e-01],\n",
       "       [ 7.17211902e-01, -7.46097207e-01],\n",
       "       [ 1.57848096e+00, -1.33859706e+00],\n",
       "       [-5.38172066e-01,  5.71654320e-01],\n",
       "       [ 9.96915758e-01, -7.54357338e-01],\n",
       "       [-4.69888777e-01,  4.90485728e-01],\n",
       "       [ 3.67970973e-01, -3.41216624e-01],\n",
       "       [ 1.88728929e-01, -1.37334645e-01],\n",
       "       [ 1.08530164e-01, -1.32542938e-01],\n",
       "       [-3.37347448e-01,  3.64986807e-01],\n",
       "       [-4.15719092e-01,  4.05188262e-01],\n",
       "       [ 5.22134542e-01, -4.98204887e-01],\n",
       "       [ 1.04848981e+00, -9.56722498e-01],\n",
       "       [ 1.54362488e+00, -1.14909935e+00],\n",
       "       [ 1.43874037e+00, -1.02619338e+00],\n",
       "       [ 6.97647750e-01, -5.73202372e-01],\n",
       "       [ 6.66157842e-01, -6.98382974e-01],\n",
       "       [ 1.48050225e+00, -1.17934549e+00],\n",
       "       [ 1.74683928e+00, -1.28825366e+00],\n",
       "       [ 1.50417221e+00, -1.34464288e+00],\n",
       "       [ 9.43550408e-01, -8.50069046e-01],\n",
       "       [ 7.93837905e-01, -6.07121229e-01],\n",
       "       [ 1.62583530e+00, -1.47988474e+00],\n",
       "       [ 1.18956137e+00, -9.98678267e-01],\n",
       "       [ 2.31851876e-01, -2.47740358e-01],\n",
       "       [ 2.94460952e-02, -5.55450767e-02],\n",
       "       [ 9.91293907e-01, -8.92888784e-01],\n",
       "       [ 1.34931874e+00, -1.18614483e+00],\n",
       "       [ 5.51548123e-01, -4.92693067e-01],\n",
       "       [ 1.87952185e+00, -1.35564566e+00],\n",
       "       [ 1.44998837e+00, -1.26280355e+00],\n",
       "       [ 1.20578384e+00, -9.80246365e-01],\n",
       "       [ 1.85087657e+00, -1.37682581e+00],\n",
       "       [ 1.34683371e+00, -1.04003906e+00],\n",
       "       [-3.74387592e-01,  4.33916003e-01],\n",
       "       [ 4.23777699e-02, -4.38921452e-02],\n",
       "       [-5.38700044e-01,  6.36709988e-01],\n",
       "       [ 2.85844654e-01, -6.09908998e-02],\n",
       "       [ 1.35872912e+00, -1.13661921e+00],\n",
       "       [ 1.00066972e+00, -8.01582396e-01],\n",
       "       [-1.96604028e-01,  2.30948284e-01],\n",
       "       [-1.18364066e-01,  6.28493428e-02],\n",
       "       [ 1.67795217e+00, -9.23294067e-01],\n",
       "       [ 5.38976192e-01, -5.25252342e-01],\n",
       "       [ 7.14940667e-01, -6.86512470e-01],\n",
       "       [ 8.61554146e-02,  7.81700015e-03],\n",
       "       [ 5.56784034e-01, -5.14672637e-01],\n",
       "       [ 6.83791101e-01, -5.50038397e-01],\n",
       "       [ 1.67234302e+00, -1.37486351e+00],\n",
       "       [ 2.37888232e-01, -1.89653009e-01],\n",
       "       [ 1.62303889e+00, -1.11576116e+00],\n",
       "       [-6.81185484e-01,  6.44164562e-01],\n",
       "       [ 1.01270914e+00, -8.88852298e-01],\n",
       "       [ 5.00892639e-01, -2.79727817e-01],\n",
       "       [ 3.11928064e-01, -1.78120911e-01],\n",
       "       [ 6.92540169e-01, -6.48149610e-01],\n",
       "       [ 5.11014402e-01, -5.23283124e-01],\n",
       "       [ 1.17521763e+00, -1.09758115e+00],\n",
       "       [ 1.48061788e+00, -1.19684815e+00],\n",
       "       [ 6.94157779e-01, -7.08211362e-01],\n",
       "       [ 8.01446736e-02, -9.19142365e-03],\n",
       "       [ 1.50570118e+00, -1.28764653e+00],\n",
       "       [ 1.07113075e+00, -9.94927049e-01],\n",
       "       [ 5.44895530e-02, -2.08032429e-02],\n",
       "       [ 7.71665037e-01, -5.05418241e-01],\n",
       "       [ 1.25629926e+00, -1.04973280e+00],\n",
       "       [ 1.51239932e-02, -3.51458788e-02],\n",
       "       [ 4.88718480e-01, -3.86516213e-01],\n",
       "       [ 1.95679069e-01, -1.49863541e-01],\n",
       "       [ 5.78481078e-01, -4.50532258e-01],\n",
       "       [-1.75949216e-01,  2.10803777e-01],\n",
       "       [-1.20105714e-01,  1.72698438e-01],\n",
       "       [ 1.33859038e+00, -8.29164982e-01],\n",
       "       [ 1.23343873e+00, -1.08492255e+00],\n",
       "       [-2.83481866e-01,  2.56230175e-01],\n",
       "       [-3.27916861e-01,  2.78554857e-01],\n",
       "       [ 8.57553184e-01, -8.20591927e-01],\n",
       "       [ 8.49255562e-01, -6.93544269e-01],\n",
       "       [ 8.66392374e-01, -8.03236067e-01],\n",
       "       [ 1.08534425e-01, -1.38525784e-01],\n",
       "       [ 1.36824870e+00, -1.12592340e+00],\n",
       "       [ 1.28128445e+00, -9.59533036e-01],\n",
       "       [ 1.49311888e+00, -1.18785143e+00],\n",
       "       [ 1.82062721e+00, -1.49351478e+00],\n",
       "       [ 3.18604380e-01, -3.05583030e-01],\n",
       "       [ 7.76170552e-01, -6.33799136e-01],\n",
       "       [ 1.34516180e+00, -1.09168220e+00],\n",
       "       [-6.19625151e-01,  6.59201682e-01],\n",
       "       [ 4.64646250e-01, -4.29159701e-01],\n",
       "       [ 1.06462634e+00, -9.43139195e-01],\n",
       "       [ 4.84665006e-01, -3.88116956e-01],\n",
       "       [ 1.57290781e+00, -1.36316490e+00],\n",
       "       [ 5.25258183e-01, -4.53062624e-01],\n",
       "       [ 1.53965008e+00, -1.37988329e+00],\n",
       "       [-4.04216200e-01,  4.03017133e-01],\n",
       "       [ 7.32240438e-01, -5.38016558e-01],\n",
       "       [ 7.64467776e-01, -6.85460925e-01],\n",
       "       [ 2.03276306e-01, -1.87756583e-01],\n",
       "       [ 4.02402997e-01, -3.88895959e-01],\n",
       "       [ 1.23905027e+00, -1.01618636e+00],\n",
       "       [ 1.11196864e+00, -9.12770987e-01],\n",
       "       [ 3.93931955e-01, -3.63071293e-01],\n",
       "       [-4.73447442e-02,  2.31048644e-01],\n",
       "       [ 1.10482848e+00, -9.06764507e-01],\n",
       "       [-3.38157117e-02,  3.86211723e-02],\n",
       "       [ 6.05635345e-02, -1.89771801e-02],\n",
       "       [ 8.55052292e-01, -7.57196605e-01],\n",
       "       [ 1.69747055e-01, -1.01989388e-01],\n",
       "       [ 3.79708976e-01,  7.38516450e-04],\n",
       "       [-1.69770762e-01,  2.07279712e-01],\n",
       "       [ 1.29065275e+00, -9.65427101e-01],\n",
       "       [-4.49582547e-01,  5.66164792e-01],\n",
       "       [ 1.44899726e-01, -1.22305110e-01],\n",
       "       [-2.18567520e-01,  3.96331429e-01],\n",
       "       [ 1.03722596e+00, -8.09920549e-01],\n",
       "       [ 1.02273238e+00, -8.70222330e-01],\n",
       "       [ 1.33202207e+00, -1.19906294e+00],\n",
       "       [ 1.40888166e+00, -1.05266714e+00],\n",
       "       [ 6.74767077e-01, -4.67683047e-01],\n",
       "       [ 7.42464483e-01, -6.89421356e-01],\n",
       "       [-7.09848940e-01,  6.84886515e-01],\n",
       "       [ 8.66699815e-01, -8.28900576e-01],\n",
       "       [ 7.45133638e-01, -6.31092310e-01],\n",
       "       [ 1.72911859e+00, -1.20647502e+00],\n",
       "       [-1.93647712e-01,  2.39813879e-01],\n",
       "       [ 1.26178944e+00, -1.01302159e+00],\n",
       "       [ 6.60413742e-01, -5.78309774e-01],\n",
       "       [ 8.94682884e-01, -7.15314150e-01],\n",
       "       [-4.87864971e-01,  5.83328784e-01],\n",
       "       [ 1.30366981e+00, -1.17225170e+00],\n",
       "       [ 9.31884646e-01, -8.58599365e-01],\n",
       "       [ 8.03902745e-03,  3.11484039e-02],\n",
       "       [ 6.56280756e-01, -5.25876999e-01],\n",
       "       [ 1.06387830e+00, -6.74672961e-01],\n",
       "       [-5.05009234e-01,  4.08600777e-01],\n",
       "       [ 7.13265657e-01, -5.89946210e-01],\n",
       "       [ 7.12599516e-01, -5.08491158e-01],\n",
       "       [ 2.16183841e-01, -1.50469691e-01],\n",
       "       [ 5.99040926e-01, -5.54197550e-01],\n",
       "       [-2.53813446e-01,  3.24903578e-01],\n",
       "       [ 1.16534984e+00, -1.01870096e+00],\n",
       "       [ 1.29278028e+00, -1.10157537e+00],\n",
       "       [ 5.13939142e-01, -5.10471761e-01],\n",
       "       [ 1.36557984e+00, -1.12421751e+00],\n",
       "       [ 5.72657764e-01,  1.09889820e-01],\n",
       "       [-6.61792815e-01,  7.27041602e-01],\n",
       "       [ 2.48454452e-01, -2.32406527e-01],\n",
       "       [ 4.91950870e-01, -3.61043811e-01],\n",
       "       [ 8.94788027e-01, -7.86096454e-01],\n",
       "       [ 9.99242783e-01, -8.68222594e-01],\n",
       "       [-1.68324262e-01,  2.19221696e-01],\n",
       "       [ 7.27293134e-01, -6.58948302e-01],\n",
       "       [ 1.79073989e+00, -1.29538631e+00],\n",
       "       [-9.31253731e-02,  1.66615501e-01],\n",
       "       [-7.20570385e-02,  1.39124840e-01],\n",
       "       [ 3.65231216e-01, -2.83066034e-01],\n",
       "       [-6.03296638e-01,  5.57468712e-01],\n",
       "       [ 1.31903064e+00, -1.13581777e+00],\n",
       "       [-5.19540131e-01,  4.56480414e-01],\n",
       "       [ 5.82700551e-01, -3.80528778e-01],\n",
       "       [ 1.35099089e+00, -1.17460573e+00],\n",
       "       [ 5.04133403e-01, -4.49167937e-01],\n",
       "       [ 6.10910952e-01, -5.56086838e-01]], dtype=float32), label_ids=array([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0]), metrics={'test_loss': 0.3267303705215454, 'test_accuracy': 0.9079601990049752, 'test_runtime': 68.192, 'test_samples_per_second': 5.895, 'test_steps_per_second': 0.191})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3be0e585-96b2-44cd-8f32-4b037fcdfe17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d68b7f8c-d3c7-44b5-8ea0-0a71ad0d0c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array(test_ds[:]['label'])\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dff4e6f8-4cbd-4725-bbdf-6ff1cbbae763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[281,  12],\n",
       "       [ 25,  84]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ad9c5-a7f4-4fc6-b788-e50415770018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0dbb1-d40d-4bdd-a39c-7c9dd7b17888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRUEBA:Python",
   "language": "python",
   "name": "conda-env-PRUEBA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
