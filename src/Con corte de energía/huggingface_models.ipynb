{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882c7827-d492-48aa-8cb6-a77614297265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we choose the pretrained model to use \n",
    "\n",
    "model_id = \"google/vit-base-patch16-224\"\n",
    "#model_id = 'microsoft/swin-tiny-patch4-window7-224'\n",
    "#model_id = 'facebook/deit-base-patch16-224'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f530c-0b87-4a91-82f4-a60bf626cdb9",
   "metadata": {},
   "source": [
    "Now we load the ViT feature extractor to process the image into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cc9044-cb70-47c5-b31f-1657558d1df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/PRUEBA/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, ViTFeatureExtractor\n",
    "#feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4166d1c-1a73-4190-8e82-624200047301",
   "metadata": {},
   "source": [
    "This feature extractor will resize every image to the resolution that the model expects and normalize channels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491140c-c230-4655-9f89-e005c4d3a426",
   "metadata": {},
   "source": [
    "We define 2 functions, one for training and one for validation, including resizing, center cropping and normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd82a8a4-462b-4f15-ab70-da22f3a57f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(feature_extractor.size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(feature_extractor.size),\n",
    "            CenterCrop(feature_extractor.size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"img\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"img\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a291a-fafd-4580-b8ca-a40b9a71640b",
   "metadata": {},
   "source": [
    "Next, we can preprocess our dataset by applying these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f48cb2-035f-4b64-844d-36911beb1f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 2666\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 297\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 524\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n",
    "ds = load_from_disk('./data_dict_cut')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300c5553-6b86-4086-ad35-a15c5875c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "train_ds = ds['train']\n",
    "val_ds = ds['val']\n",
    "test_ds = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "256e4b28-52e6-4572-886a-a2aaaf814b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gamma', 'iron', 'proton']\n"
     ]
    }
   ],
   "source": [
    "#Classes names\n",
    "labels = train_ds.features[\"label\"].names\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d151d8-791d-4eaf-ac20-0e22aab9454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'transform'=<function preprocess_train at 0x7f7a2fc373a0> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfc4ed91-ffab-4e6f-bf0e-e90db0e8b8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=288x288>,\n",
       " 'label': 2,\n",
       " 'pixel_values': tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.9686, 0.9922, 1.0000,  ..., 0.9922, 0.9686, 0.9059],\n",
       "          [0.7176, 0.8431, 0.9529,  ..., 0.9922, 0.9765, 0.9765],\n",
       "          [0.7725, 0.8039, 0.8824,  ..., 0.8588, 0.6549, 0.9608]],\n",
       " \n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.9451, 0.9843, 0.9922,  ..., 0.9843, 0.9529, 0.8902],\n",
       "          [0.6863, 0.8118, 0.9294,  ..., 0.9843, 0.9686, 0.9608],\n",
       "          [0.7412, 0.7725, 0.8510,  ..., 0.8431, 0.6392, 0.9451]],\n",
       " \n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.9529, 0.9922, 1.0000,  ..., 0.9922, 0.9608, 0.8980],\n",
       "          [0.6941, 0.8196, 0.9373,  ..., 0.9922, 0.9765, 0.9686],\n",
       "          [0.7490, 0.7804, 0.8588,  ..., 0.8510, 0.6471, 0.9529]]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaa161-cffe-4796-8012-91e66d879fd1",
   "metadata": {},
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. We use the modelViTForImageClassification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22aa812a-f91f-4b38-8b18-eab8d71163f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a dictionary that maps a label name to an integer and vice versa. \n",
    "#The mapping will help the model recover the label name from the label number.\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43fba3aa-f533-4ba3-ae49-a57b117002d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(model_id,\n",
    "                                                 label2id=label2id,\n",
    "                                                 id2label=id2label,\n",
    "                                                ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3e17f-979c-47ca-bad5-76f087d1b333",
   "metadata": {},
   "source": [
    "The warning is telling us we are throwing away some weights (the weights and bias of the classifier layer) and randomly initializing some other (the weights and bias of a new classifier layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7ce9b-a0f0-4f10-90a1-77b8e5613dd1",
   "metadata": {},
   "source": [
    "To instantiate a Trainer, we will need to define the training configuration and the evaluation metric. The most important is the TrainingArguments, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model.\n",
    "\n",
    "Most of the training arguments are pretty self-explanatory, but one that is quite important here is remove_unused_columns=False. This one will drop any features not used by the model's call function. By default it's True because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in our case, we need the unused features ('image' in particular) in order to create 'pixel_values'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ede566e3-36e9-473d-baeb-30b0c2cf2ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_id.split(\"/\")[-1]\n",
    "batch_size = 32\n",
    "learning_rate = 5e-5\n",
    "gradient_accumulation_steps = 4\n",
    "epochs = 3\n",
    "warmup_ratio= 0.1\n",
    "logging_steps=10\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ds\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    logging_steps=logging_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "   # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1464cb-4b0a-4621-b65a-6056e36cad1c",
   "metadata": {},
   "source": [
    "Next, we need to define a function for how to compute the metrics from the predictions, which will just use the metric we loaded earlier. Let us also load the Accuracy metric, which we'll use to evaluate our model both during and after training. The only preprocessing we have to do is to take the argmax of our predicted logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be8d4103-623c-41be-816f-71aba0fad059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2b985-b913-4849-b3f4-2fdbe77fe455",
   "metadata": {},
   "source": [
    "We also define a collate_fn, which will be used to batch examples together. Each batch consists of 2 keys, namely pixel_values and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "649a749c-d111-47f8-807b-0ba04b53d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b265339-7eec-44e3-8c38-1ce884527253",
   "metadata": {},
   "source": [
    "Then we just need to pass all of this along with our datasets to the Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5780f1c8-20f9-4694-b90a-099b5b02a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9bc7b0-35ce-496f-a8ef-27f59326e421",
   "metadata": {},
   "source": [
    "Now we can finetune our model by calling the train method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9afe1d3f-c7e2-47a8-9411-5d8509d89f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/PRUEBA/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2666\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 1:12:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.189700</td>\n",
       "      <td>0.174817</td>\n",
       "      <td>0.952862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.037598</td>\n",
       "      <td>0.983165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.035037</td>\n",
       "      <td>0.983165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 297\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds/checkpoint-21\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/checkpoint-21/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/checkpoint-21/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/checkpoint-21/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 297\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds/checkpoint-42\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/checkpoint-42/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/checkpoint-42/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/checkpoint-42/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 297\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds/checkpoint-63\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/checkpoint-63/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/checkpoint-63/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/checkpoint-63/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from vit-base-patch16-224-finetuned-ds/checkpoint-42 (score: 0.9831649831649831).\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         3.0\n",
      "  total_flos               = 577221147GF\n",
      "  train_loss               =       0.168\n",
      "  train_runtime            =  1:14:09.28\n",
      "  train_samples_per_second =       1.798\n",
      "  train_steps_per_second   =       0.014\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "# rest is optional but nice to have\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70610cf9-8570-4b37-b43c-bdc964a2c8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 297\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 02:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_accuracy           =     0.9832\n",
      "  eval_loss               =     0.0376\n",
      "  eval_runtime            = 0:00:49.49\n",
      "  eval_samples_per_second =        6.0\n",
      "  eval_steps_per_second   =      0.202\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = trainer.evaluate()\n",
    "# some nice to haves:\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43c9156b-84a2-4d28-875f-6c9796f41d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96167b9c-d84b-43c0-a85f-fae3ccf58ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 524\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "outputs = trainer.predict(test_ds)\n",
    "y_pred = outputs.predictions.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a35c48e8-a78e-47d1-85f8-97fc7a5c91c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9885496183206107}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f847d7ab-5acd-454d-af4d-044049cab94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.2629285 , -1.229901  ,  5.3129826 ],\n",
       "       [-2.4262114 , -0.37825078,  5.01449   ],\n",
       "       [ 3.980997  , -2.4187832 , -0.01041143],\n",
       "       ...,\n",
       "       [-2.5934508 , -0.69922996,  5.075904  ],\n",
       "       [-2.118897  , -1.1352144 ,  5.019837  ],\n",
       "       [-2.8464427 , -0.2949299 ,  5.3533435 ]], dtype=float32), label_ids=array([2, 2, 0, 1, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2]), metrics={'test_loss': 0.03296665847301483, 'test_accuracy': 0.9885496183206107, 'test_runtime': 86.7489, 'test_samples_per_second': 6.04, 'test_steps_per_second': 0.196})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3be0e585-96b2-44cd-8f32-4b037fcdfe17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d68b7f8c-d3c7-44b5-8ea0-0a71ad0d0c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 1, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array(test_ds[:]['label'])\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dff4e6f8-4cbd-4725-bbdf-6ff1cbbae763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70,   0,   0],\n",
       "       [  0,   0,   6],\n",
       "       [  0,   0, 448]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b0049-0f6a-4710-9eb4-6d2f76aacce5",
   "metadata": {},
   "source": [
    "It fails all the examples from the minority class. Now, we are going to use some technics to balance our data. First we try oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9544b81f-601f-4c8e-8b64-9a21b4b112f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_598/3257190274.py:4: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  train_ds_ros = ros.fit_resample(np.array(train_ds[:]['pixel_values']).reshape(-1, 1), train_ds[:]['label'])\n",
      "/tmp/ipykernel_598/3257190274.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_ds_ros = ros.fit_resample(np.array(train_ds[:]['pixel_values']).reshape(-1, 1), train_ds[:]['label'])\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "train_ds_ros = ros.fit_resample(np.array(train_ds[:]['pixel_values']).reshape(-1, 1), train_ds[:]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a1c0a44-1e17-4527-adb5-06c2d0f5cd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2278.,    0.,    0.,    0.,    0., 2278.,    0.,    0.,    0.,\n",
       "        2278.]),\n",
       " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_ds_ros[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64665688-4d8b-4f1e-84cf-a0fd10dc9cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/PRUEBA/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2666\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 1:09:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>0.223251</td>\n",
       "      <td>0.912458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.025621</td>\n",
       "      <td>0.986532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.036804</td>\n",
       "      <td>0.986532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 297\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds/checkpoint-21\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/checkpoint-21/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/checkpoint-21/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/checkpoint-21/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 297\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds/checkpoint-42\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/checkpoint-42/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/checkpoint-42/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/checkpoint-42/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 297\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds/checkpoint-63\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/checkpoint-63/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/checkpoint-63/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/checkpoint-63/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from vit-base-patch16-224-finetuned-ds/checkpoint-42 (score: 0.9865319865319865).\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         3.0\n",
      "  total_flos               = 577221147GF\n",
      "  train_loss               =      0.1234\n",
      "  train_runtime            =  1:10:10.42\n",
      "  train_samples_per_second =         1.9\n",
      "  train_steps_per_second   =       0.015\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "# rest is optional but nice to have\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9c809f9-ed8b-4020-ac7f-0c29cbeccd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ca2087b-dd3d-400c-87c1-de54d2a88c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 524\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 01:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = trainer.predict(test_ds)\n",
    "y_pred = outputs.predictions.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "614baf5c-e14c-4f5f-916a-811bce4f24c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.6410716 , -2.481266  ,  4.1812963 ],\n",
       "       [-2.874542  , -2.2528222 ,  4.2495956 ],\n",
       "       [ 3.0468779 , -3.6443503 ,  0.19668053],\n",
       "       ...,\n",
       "       [-2.8706894 , -2.3254006 ,  4.3293877 ],\n",
       "       [-2.5220096 , -2.4751878 ,  4.3587317 ],\n",
       "       [-2.9838305 , -2.2278    ,  4.486712  ]], dtype=float32), label_ids=array([2, 2, 0, 1, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2]), metrics={'test_loss': 0.033281709998846054, 'test_accuracy': 0.982824427480916, 'test_runtime': 86.958, 'test_samples_per_second': 6.026, 'test_steps_per_second': 0.195})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ee71aa7-9efd-4abc-bf58-75431df9955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(test_ds[:]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb8286e2-5fa7-4c59-aa86-b5fc08e6faf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 67,   0,   3],\n",
       "       [  0,   0,   6],\n",
       "       [  0,   0, 448]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9abb369-ee70-4eeb-9399-dff7c1c7c9ec",
   "metadata": {},
   "source": [
    "This does not work. We are going to try changing the classes weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24fe6133-74ef-4cdf-adb0-0a665d2542ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                         classes=np.unique(train_ds[:]['label']),\n",
    "                                                         y=train_ds[:]['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab52dc0c-14c3-42b3-aee3-10d25caada00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.50328638, 26.92929293,  0.39010828])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afd48af4-1c17-4f31-aebb-fee94bb396a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([2.5,26.9,0.4]))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32d1251e-a983-416b-bc9d-c965abc5f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5973e63-2bbd-47e5-84b1-afb343d58115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/PRUEBA/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2666\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 1:18:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>0.623527</td>\n",
       "      <td>0.646465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.047802</td>\n",
       "      <td>0.989899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.027302</td>\n",
       "      <td>0.986532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 297\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds/checkpoint-21\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/checkpoint-21/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/checkpoint-21/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/checkpoint-21/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 297\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds/checkpoint-42\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/checkpoint-42/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/checkpoint-42/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/checkpoint-42/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 297\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-finetuned-ds/checkpoint-63\n",
      "Configuration saved in vit-base-patch16-224-finetuned-ds/checkpoint-63/config.json\n",
      "Model weights saved in vit-base-patch16-224-finetuned-ds/checkpoint-63/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-finetuned-ds/checkpoint-63/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from vit-base-patch16-224-finetuned-ds/checkpoint-42 (score: 0.98989898989899).\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31e013bc-9183-4849-9989-68140da126c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c34b1c4-3cf7-4d5e-ba30-e87684b60fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 524\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 01:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = trainer.predict(test_ds)\n",
    "y_pred = outputs.predictions.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca4b537f-f9b7-4a38-8630-5e92be3577b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 69,   0,   1],\n",
       "       [  0,   5,   1],\n",
       "       [  1,   0, 447]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array(test_ds[:]['label'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcca6c-dc5b-4b98-a424-8c98b50f885d",
   "metadata": {},
   "source": [
    "Now it classifies examples from the minoritary class. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRUEBA:Python",
   "language": "python",
   "name": "conda-env-PRUEBA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
