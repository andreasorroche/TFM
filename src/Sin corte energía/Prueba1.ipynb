{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b29a441-f93b-4b2c-bdf3-55c9c5857971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset\n",
    "import os\n",
    "import datasets\n",
    "\n",
    "model_id = \"google/vit-base-patch16-224\"\n",
    "\n",
    "def create_image_folder_dataset(root_path):\n",
    "  \"\"\"creates `Dataset` from image folder structure\"\"\"\n",
    "\n",
    "  # get class names by folders names\n",
    "  _CLASS_NAMES= os.listdir(root_path)\n",
    "  # defines `datasets` features`\n",
    "  features=datasets.Features({\n",
    "                      \"img\": datasets.Image(),\n",
    "                      \"label\": datasets.features.ClassLabel(names=_CLASS_NAMES),\n",
    "                  })\n",
    "  # temp list holding datapoints for creation\n",
    "  img_data_files=[]\n",
    "  label_data_files=[]\n",
    "  # load images into list for creation\n",
    "  for img_class in os.listdir(root_path):\n",
    "    for img in os.listdir(os.path.join(root_path,img_class)):\n",
    "      path_=os.path.join(root_path,img_class,img)\n",
    "      img_data_files.append(path_)\n",
    "      label_data_files.append(img_class)\n",
    "  # create dataset\n",
    "  ds = datasets.Dataset.from_dict({\"img\":img_data_files,\"label\":label_data_files},features=features)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e14d2a-0dd2-4f04-8f0e-6c0392bfcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = create_image_folder_dataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd96aff0-afa7-4e54-b28a-8193dd15cbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['img', 'label'],\n",
       "    num_rows: 6402\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1d89a2-8bf3-4424-bdaf-53e6a96ae4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gamma', 'iron', 'proton']\n"
     ]
    }
   ],
   "source": [
    "#Classes names\n",
    "labels = ds.features[\"label\"].names\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a694f264-3d49-484b-be80-940b05803fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test size will be 15% of train dataset\n",
    "test_size=.15\n",
    "\n",
    "ds_split = ds.shuffle().train_test_split(test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c8d315-50d5-423d-9281-82b90f817c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 5441\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 961\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2eb1d1a-ad1c-4b17-8c60-e0ecc7c611b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=288x288>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_split['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd8163c-b774-4c77-bafa-291c097435dd",
   "metadata": {},
   "source": [
    "We take a look at an example. The image field contains a PIL image and each label is an integer that represents a class. We create a dictionary that maps a label name to an integer and vice versa. The mapping will help the model recover the label name from the label number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5255fe2-7300-496a-af2f-051832ba22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ab31d-cefa-43ce-8167-ac237bb8ea41",
   "metadata": {},
   "source": [
    "Now we can covert the label number to a label name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba5edd1-942d-46b8-89b3-5ca5f3a4b9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gamma'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[str(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "431fb3e9-9d75-4890-83b4-3b0e77e5c286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iron'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[str(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8e1baf9-6c34-4fb6-b308-3fc15c4f91c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proton'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[str(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f530c-0b87-4a91-82f4-a60bf626cdb9",
   "metadata": {},
   "source": [
    "Now we load the ViT feature extractor to process the image into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02cc9044-cb70-47c5-b31f-1657558d1df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "model_id = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4166d1c-1a73-4190-8e82-624200047301",
   "metadata": {},
   "source": [
    "This feature extractor will resize every image to the resolution that the model expects and normalize channels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491140c-c230-4655-9f89-e005c4d3a426",
   "metadata": {},
   "source": [
    "We define 2 functions, one for training and one for validation, including resizing, center cropping and normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd82a8a4-462b-4f15-ab70-da22f3a57f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(feature_extractor.size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(feature_extractor.size),\n",
    "            CenterCrop(feature_extractor.size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"img\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"img\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a291a-fafd-4580-b8ca-a40b9a71640b",
   "metadata": {},
   "source": [
    "Next, we can preprocess our dataset by applying these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "300c5553-6b86-4086-ad35-a15c5875c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "splits = ds_split[\"train\"].train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97d151d8-791d-4eaf-ac20-0e22aab9454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'transform'=<function preprocess_train at 0x7fca989788b0> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfc4ed91-ffab-4e6f-bf0e-e90db0e8b8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=288x288>,\n",
       " 'label': 2,\n",
       " 'pixel_values': tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9373,  0.9216,  0.9137],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9294,  0.9294,  0.9216],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9294,  0.9294,  0.9294]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.1059, -0.1216, -0.1294],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.0980, -0.1137, -0.1216],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.0980, -0.1059, -0.1137]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.2392, -0.2549, -0.2627],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.2392, -0.2471, -0.2549],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.2392, -0.2471, -0.2471]]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaa161-cffe-4796-8012-91e66d879fd1",
   "metadata": {},
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. We use the modelViTForImageClassification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43fba3aa-f533-4ba3-ae49-a57b117002d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(model_id,\n",
    "                                                 label2id=label2id,\n",
    "                                                 id2label=id2label,\n",
    "                                                # ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3e17f-979c-47ca-bad5-76f087d1b333",
   "metadata": {},
   "source": [
    "The warning is telling us we are throwing away some weights (the weights and bias of the classifier layer) and randomly initializing some other (the weights and bias of a new classifier layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7ce9b-a0f0-4f10-90a1-77b8e5613dd1",
   "metadata": {},
   "source": [
    "To instantiate a Trainer, we will need to define the training configuration and the evaluation metric. The most important is the TrainingArguments, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model.\n",
    "\n",
    "Most of the training arguments are pretty self-explanatory, but one that is quite important here is remove_unused_columns=False. This one will drop any features not used by the model's call function. By default it's True because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in our case, we need the unused features ('image' in particular) in order to create 'pixel_values'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ede566e3-36e9-473d-baeb-30b0c2cf2ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_id.split(\"/\")[-1]\n",
    "batch_size = 32\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ds\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "   # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1464cb-4b0a-4621-b65a-6056e36cad1c",
   "metadata": {},
   "source": [
    "Next, we need to define a function for how to compute the metrics from the predictions, which will just use the metric we loaded earlier. Let us also load the Accuracy metric, which we'll use to evaluate our model both during and after training. The only preprocessing we have to do is to take the argmax of our predicted logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be8d4103-623c-41be-816f-71aba0fad059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2b985-b913-4849-b3f4-2fdbe77fe455",
   "metadata": {},
   "source": [
    "We also define a collate_fn, which will be used to batch examples together. Each batch consists of 2 keys, namely pixel_values and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "649a749c-d111-47f8-807b-0ba04b53d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b265339-7eec-44e3-8c38-1ce884527253",
   "metadata": {},
   "source": [
    "Then we just need to pass all of this along with our datasets to the Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5780f1c8-20f9-4694-b90a-099b5b02a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9bc7b0-35ce-496f-a8ef-27f59326e421",
   "metadata": {},
   "source": [
    "Now we can finetune our model by calling the train method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9afe1d3f-c7e2-47a8-9411-5d8509d89f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4896\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 114\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 2:53:18, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.278600</td>\n",
       "      <td>0.200907</td>\n",
       "      <td>0.963303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.090200</td>\n",
       "      <td>0.068930</td>\n",
       "      <td>0.994495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>0.071080</td>\n",
       "      <td>0.992661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 545\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-ds/checkpoint-38\n",
      "Configuration saved in vit-base-patch16-224-in21k-finetuned-ds/checkpoint-38/config.json\n",
      "Model weights saved in vit-base-patch16-224-in21k-finetuned-ds/checkpoint-38/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-in21k-finetuned-ds/checkpoint-38/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 545\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-ds/checkpoint-76\n",
      "Configuration saved in vit-base-patch16-224-in21k-finetuned-ds/checkpoint-76/config.json\n",
      "Model weights saved in vit-base-patch16-224-in21k-finetuned-ds/checkpoint-76/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-in21k-finetuned-ds/checkpoint-76/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 545\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-ds/checkpoint-114\n",
      "Configuration saved in vit-base-patch16-224-in21k-finetuned-ds/checkpoint-114/config.json\n",
      "Model weights saved in vit-base-patch16-224-in21k-finetuned-ds/checkpoint-114/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-in21k-finetuned-ds/checkpoint-114/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from vit-base-patch16-224-in21k-finetuned-ds/checkpoint-76 (score: 0.9944954128440368).\n",
      "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-ds\n",
      "Configuration saved in vit-base-patch16-224-in21k-finetuned-ds/config.json\n",
      "Model weights saved in vit-base-patch16-224-in21k-finetuned-ds/pytorch_model.bin\n",
      "Feature extractor saved in vit-base-patch16-224-in21k-finetuned-ds/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         2.99\n",
      "  total_flos               = 1057733575GF\n",
      "  train_loss               =       0.2282\n",
      "  train_runtime            =   2:54:51.91\n",
      "  train_samples_per_second =          1.4\n",
      "  train_steps_per_second   =        0.011\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "# rest is optional but nice to have\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70610cf9-8570-4b37-b43c-bdc964a2c8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 545\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43c9156b-84a2-4d28-875f-6c9796f41d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ds_split['test']\n",
    "test_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96167b9c-d84b-43c0-a85f-fae3ccf58ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 961\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 25:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = trainer.predict(test_ds)\n",
    "y_pred = outputs.predictions.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a35c48e8-a78e-47d1-85f8-97fc7a5c91c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9968782518210197}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847d7ab-5acd-454d-af4d-044049cab94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
